## Самостоятельная работа 3-1. Работа в HDFS в экосистеме cloudera
### Цель работы
Изучение основных операций для работы с распределенной файловой системой HDFS.
### Задачи работы
* подготовка окружения,
* запуск shell-клиента,
* изучение основных shell-команд,

3.1.1. Развернуть виртуальное окружение.

3.1.2. Вывести с помощью команды `help` описание основных команды shell-клиента.

3.1.3. Просмотреть корневую директорию `HDFS`.

3.1.4. Создать в HDFS в директории `/user/mgpu` поддиректорию `ваше_фио`.

3.1.5. Создать в локальной файловой системе случайный текстовый файл размером 10 Mb с именем, образованным вашими инициалами
base64 /dev/urandom | head -c 10000000 > file.txt .

3.1.6. Заархивировать созданный текстовый файл gzip -c file.txt > file.gz .

3.1.7. Скопировать текстовый файл и архив в директорию /user/mgpu/fio HDFS виртуальной машины.

3.1.8. Просмотреть файл и архив с помощью утилит `cat`, `text` в комбинации с каналами и утилитами `head`, `tail` -- привести не менее 3 вариантов команд и просмотра файла. 

3.1.9. Создать копию файла file.txt вида date_file.txt, где в начале имени файла-копии указана текущая дата. Вывести листинг.

3.1.10. Вывести статистику по директории `/user/mgpu/fio` виртуальной машины.

3.1.11. Удалить поддиректорию `/fio` со всем содержимым.

Если в системе ещё нет /dev/random и /dev/urandom, то их можно создать следующими командами:

- mknod -m 644 /dev/random c 1 8 

- mknod -m 644 /dev/urandom c 1 9 

- chown root:root /dev/random /dev/urandom 

Перед выполнением команд желательно создать контрольную точку (в HyperV) для восстановления машины до применения команд. 

3.1.12. Подсчитать количество слов в файле внутри HDFS с помощью методологии `Map Reduce` (размер файла не менее 128 Мб).

`3.1.13` Создание таблицы в `Hive`
1.	Скачать [датасет](https://github.com/BosenkoTM/cloudera-quickstart/blob/main/data/athlete.snappy.parquet) или [тут](https://storage.googleapis.com/otus_sample_data/athlete.snappy.parquet) 
3.	Через `HUE` загрузите файл в папку `/user/cloudera/athlete`.
4. В навигационном меню выберите `Files`.
5. Создайте папку.
6. Загрузите файл в `HDFS`, нажав `Upload`.
7.	Перейдите в “Editor > Hive” и выполните запрос:

```sql
CREATE EXTERNAL TABLE athlete (
    ID INT,
    Name STRING,
    Sex STRING,
    Age INT,
    Height INT,
    Weight INT,
    Team STRING,
    NOC STRING,
    Games STRING,
    `Year` INT,
    Season STRING,
    City STRING,
    Sport STRING,
    Event STRING,
    Medal STRING 
)
STORED AS PARQUET
LOCATION '/user/cloudera/athlete'
```
- Sql запрос и результаты запроса отобразить в отчете.

`3.1.14` Проанализировать и визуализировать данные с помощью `Impala`(высокоскоростной механизм запросов SQL) или `Hive`. 
- Загрузить и разархивировать [babs_open_data_year_1.zip](https://disk.yandex.ru/d/JrboaizPXSh0Mg).
- Перенести данные `201402_trip_data.csv` в `HDFS`.
- Создать таблицу в Hive  с привязкой к внешним данным `201402_trip_data.csv`.
- выполнить запрос
```bash
select `startstation`, `endstation`, count(*) as trips 
from `default`.`201402_trip_data` 
group by `startstation`, `endstation` 
order by trips desc;
```
- Создать гистограмму, щелкнув значок «Hue Bar»:
- Установить ось X в качестве начальной станции, а ось Y — в качестве маршрута.
Установить лимит `10`.
- Выгрузить результаты, выбрав `CSV` или `Excel`.

## Результаты работы представить в виде файла ФИО-3-01.pdf (выгрузить в `moodle`), в котором отражены следующие результаты:
- постановка задачи;
- скрины хода выполнения работы при выполнении `Задание 3-01.1` - `Задание 3-01.14` ;
- прикрепить также следующие файлы:
  -  лог-файл работы `HDFS`;
  - скрины, подтверждающие выпознение работы в `Hadoop`.
  - Все выполненные команды оформить отдельным файлом  в формате `ФИО-3-01_группа.txt`.
